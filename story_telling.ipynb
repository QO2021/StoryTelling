{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu9r1pphnmUT"
      },
      "source": [
        "# AI Agents Capstone Project: Dynamic Blog Writer\n",
        "## Building a Multi-Agent System Step by Step\n",
        "\n",
        "**Course**: AI Explorers: Introduction to AI Agents  \n",
        "**Project Type**: Capstone - 5 Progressive Exercises  \n",
        "**Learning Objective**: Apply the ReAct framework and agentic workflows to build a complete multi-agent system\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Course Concepts Review\n",
        "\n",
        "Before starting, let's review the key concepts from [Session 3 Notes](session_course_notes.html):\n",
        "\n",
        "### The Agent Formula\n",
        "**üß† A Brain (LLM) + üß∞ A Set of Tools = ü§ñ An AI Agent**\n",
        "\n",
        "### The ReAct Framework\n",
        "- **Thought**: The agent reasons about the problem\n",
        "- **Action**: The agent chooses and uses a tool\n",
        "- **Observation**: The agent receives results and continues the loop\n",
        "\n",
        "### Agentic Workflow Pattern: Evaluator-Optimizer\n",
        "One agent creates a draft, another agent critiques it, creating a loop of continuous improvement.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Project Overview\n",
        "\n",
        "You'll build a **Dynamic Blog Writer** that uses 5 specialized agents working together:\n",
        "\n",
        "1. **Persona Architect** üé≠ - Creates writer personas and search strategies\n",
        "2. **Research Analyst** üîç - Conducts web research and gathers information  \n",
        "3. **Content Synthesizer** ‚úçÔ∏è - Writes blog posts based on persona and research\n",
        "4. **Critic** üîç - Evaluates drafts against quality principles\n",
        "5. **Editor** ‚úèÔ∏è - Provides feedback and manages the iterative improvement process\n",
        "\n",
        "This demonstrates the **Evaluator-Optimizer** workflow pattern you learned about!\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Setup Instructions\n",
        "\n",
        "### Option 1: Google Colab (Recommended)\n",
        "1. **Open this notebook** in Google Colab\n",
        "2. **Add API Key Secret**:\n",
        "   - Click the üîë key icon in the left sidebar\n",
        "   - Add secret: `GEMINI_API_KEY`\n",
        "   - Get your key from: https://ai.google.dev/\n",
        "   - **Enable notebook access** for the secret\n",
        "3. **Run Setup**: Execute all setup cells in order\n",
        "4. **Start Coding**: Begin with Exercise 1\n",
        "\n",
        "### Option 2: Local Environment (VS Code, Cursor, etc.)\n",
        "1. **Clone/Download** this project to your local machine\n",
        "2. **Install Dependencies**:\n",
        "   ```bash\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "3. **Set API Key** (choose one method):\n",
        "   - **Environment Variable**: `export GEMINI_API_KEY=your_key_here`\n",
        "   - **VS Code Settings**: Add to your workspace settings\n",
        "   - **Direct Input**: The notebook will prompt you for the key\n",
        "4. **Launch Jupyter**:\n",
        "   ```bash\n",
        "   jupyter notebook agent_capstone_exercises.ipynb\n",
        "   ```\n",
        "5. **Start Coding**: Execute setup cells and begin with Exercise 1\n",
        "\n",
        "### Troubleshooting\n",
        "- **API Key Issues**: Ensure your Gemini API key is valid and properly set\n",
        "- **Package Issues**: Run `pip install --upgrade -r requirements.txt`\n",
        "- **Import Errors**: Restart your kernel/runtime after installing packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3EHjBEQWvtxY",
        "outputId": "b7c4bcee-d984-420d-8a4e-0a931887f0aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring pathlib2: markers 'python_version < \"3.4\"' don't match your environment\n",
            "Requirement already satisfied: google-generativeai>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.8.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.13.4)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.32.4)\n",
            "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 15))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting ipykernel>=6.25.0 (from -r requirements.txt (line 16))\n",
            "  Downloading ipykernel-6.30.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting ipython>=8.12.0 (from -r requirements.txt (line 19))\n",
            "  Downloading ipython-9.4.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (3.8.2)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (1.1.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.7.0->-r requirements.txt (line 5)) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (1.26.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.12.0->-r requirements.txt (line 8)) (2.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 9)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 9)) (2025.8.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 15)) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 15)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 15)) (7.16.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 15)) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading jupyterlab-4.4.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel>=6.25.0->-r requirements.txt (line 16))\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (1.8.15)\n",
            "Collecting jupyter-client>=8.0.0 (from ipykernel>=6.25.0->-r requirements.txt (line 16))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (5.8.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio>=1.4 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (1.6.0)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (25.0)\n",
            "Requirement already satisfied: psutil>=5.7 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=25 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.25.0->-r requirements.txt (line 16)) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=8.12.0->-r requirements.txt (line 19)) (4.4.2)\n",
            "Collecting ipython-pygments-lexers (from ipython>=8.12.0->-r requirements.txt (line 19))\n",
            "  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=8.12.0->-r requirements.txt (line 19))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=8.12.0->-r requirements.txt (line 19)) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=8.12.0->-r requirements.txt (line 19)) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=8.12.0->-r requirements.txt (line 19)) (2.19.2)\n",
            "Collecting stack_data (from ipython>=8.12.0->-r requirements.txt (line 19))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel>=6.25.0->-r requirements.txt (line 16))\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (4.9.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=8.12.0->-r requirements.txt (line 19)) (0.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=8.0.0->ipykernel>=6.25.0->-r requirements.txt (line 16)) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.25.0->-r requirements.txt (line 16)) (4.3.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=8.12.0->-r requirements.txt (line 19)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=8.12.0->-r requirements.txt (line 19)) (0.2.13)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (4.2.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 15)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 15)) (3.0.15)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (3.1.6)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading jupyter_lsp-2.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (75.2.0)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 15)) (25.1.0)\n",
            "INFO: pip is looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting notebook (from jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Collecting executing>=1.2.0 (from stack_data->ipython>=8.12.0->-r requirements.txt (line 19))\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack_data->ipython>=8.12.0->-r requirements.txt (line 19))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack_data->ipython>=8.12.0->-r requirements.txt (line 19))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.22.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (4.25.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 15)) (2.21.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.7.0->-r requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel>=6.25.0->-r requirements.txt (line 16)) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->-r requirements.txt (line 15)) (25.1.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (0.27.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (3.0.0)\n",
            "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15)) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r requirements.txt (line 15)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r requirements.txt (line 15)) (2.22)\n",
            "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 15))\n",
            "  Downloading types_python_dateutil-2.9.0.20250809-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading ipykernel-6.30.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.5/117.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-9.4.0-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m611.0/611.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading jupyterlab-4.4.6-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading notebook-7.4.5-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading jupyter_lsp-2.2.6-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250809-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: pure-eval, uri-template, types-python-dateutil, traitlets, rfc3986-validator, rfc3339-validator, python-json-logger, lark, json5, jedi, ipython-pygments-lexers, fqdn, executing, comm, async-lru, asttokens, stack_data, rfc3987-syntax, jupyter-server-terminals, arrow, jupyter-client, isoduration, ipython, ipykernel, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.7.1\n",
            "    Uninstalling traitlets-5.7.1:\n",
            "      Successfully uninstalled traitlets-5.7.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.17.1\n",
            "    Uninstalling ipykernel-6.17.1:\n",
            "      Successfully uninstalled ipykernel-6.17.1\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 6.5.7\n",
            "    Uninstalling notebook-6.5.7:\n",
            "      Successfully uninstalled notebook-6.5.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==6.17.1, but you have ipykernel 6.30.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires jupyter-server==1.16.0, but you have jupyter-server 2.17.0 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires notebook<7.0,>=5.7.6, but you have notebook 7.4.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 asttokens-3.0.0 async-lru-2.0.5 comm-0.2.3 executing-2.2.0 fqdn-1.5.1 ipykernel-6.30.1 ipython-9.4.0 ipython-pygments-lexers-1.1.1 isoduration-20.11.0 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.6 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.6 jupyterlab-server-2.27.3 lark-1.2.2 notebook-7.4.5 pure-eval-0.2.3 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 stack_data-0.6.3 traitlets-5.14.3 types-python-dateutil-2.9.0.20250809 uri-template-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "traitlets"
                ]
              },
              "id": "d52a312a0c6248949f589a3bb2338f77"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEe239XLvVbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISWu2W0GnmUb"
      },
      "source": [
        "## Dependencies and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMlMHR_VnmUc",
        "outputId": "5e60b380-2ab4-4843-d08f-d7f88ea2fd2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing google-generativeai...\n",
            "Installing beautifulsoup4...\n",
            "All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install required packages\n",
        "install_package('google-generativeai')\n",
        "install_package('beautifulsoup4')\n",
        "install_package('requests')\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp6bCUD0nmUe",
        "outputId": "e7355b21-56e7-46ae-f5df-85bf026eebf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import google.generativeai as genai\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w1psGLlnmUf",
        "outputId": "7e15145a-e901-45fb-a685-d6f8c1c074bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using Google Colab secrets\n",
            "ü§ñ Gemini Pro 2.5 model initialized successfully!\n",
            "üìù Ready to start building your multi-agent system!\n"
          ]
        }
      ],
      "source": [
        "# Configuration - API Key Setup\n",
        "try:\n",
        "    # Try Google Colab secrets first\n",
        "    from google.colab import userdata\n",
        "    gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"‚úÖ Using Google Colab secrets\")\n",
        "except ImportError:\n",
        "    # Fallback for local Jupyter\n",
        "    gemini_api_key = os.environ.get('GEMINI_API_KEY')\n",
        "    if not gemini_api_key:\n",
        "        gemini_api_key = input(\"Enter your Gemini API key: \")\n",
        "    print(\"‚úÖ Using local environment/input\")\n",
        "\n",
        "# Configure Gemini\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"‚ùå GEMINI_API_KEY not found. Please add it to Colab secrets or environment variables.\")\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "model = genai.GenerativeModel('gemini-2.5-pro')\n",
        "\n",
        "print(\"ü§ñ Gemini Pro 2.5 model initialized successfully!\")\n",
        "print(\"üìù Ready to start building your multi-agent system!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu-4k_bnnmUg"
      },
      "source": [
        "## Data Classes and Quality Principles\n",
        "\n",
        "First, let's define the data structures and quality principles that our agents will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-gNEy1ejnmUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d4c876-c205-490a-e62a-e5a3481578f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Data classes defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Data classes for our multi-agent system\n",
        "@dataclass\n",
        "class PersonaResult:\n",
        "    persona_prompt: str\n",
        "    search_queries: List[str]\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    title: str\n",
        "    url: str\n",
        "    snippet: str\n",
        "\n",
        "@dataclass\n",
        "class ResearchResult:\n",
        "    content: str\n",
        "    source_count: int = 0\n",
        "\n",
        "@dataclass\n",
        "class BlogDraft:\n",
        "    content: str\n",
        "    version: int\n",
        "\n",
        "@dataclass\n",
        "class QualityReview:\n",
        "    is_approved: bool\n",
        "    feedback: str\n",
        "    issues_found: List[str]\n",
        "\n",
        "@dataclass\n",
        "class EditorReview:\n",
        "    is_approved: bool\n",
        "    comments: str\n",
        "\n",
        "print(\"üìã Data classes defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u9wE0hW_nmUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e01c96b-ba3c-4ed9-a4da-ab0a545603aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Quality principles established:\n",
            "\n",
            "FUNDAMENTAL PRINCIPLES OF QUALITY WRITING:\n",
            "\n",
            "P1: Evidentiary Support\n",
            "All claims must be directly traceable to the provided research material.\n",
            "\n",
            "P2: Clarity and Conciseness\n",
            "Writing must be precise, unambiguous, and free of unnecessary jargon.\n",
            "\n",
            "P3: Engaging Narrative\n",
            "The post must feature a strong hook, a logical flow, and a memorable conclusion.\n",
            "\n",
            "P4: Structural Integrity\n",
            "The output must be well-organized with a clear title, introduction, body, and conclusion.\n",
            "\n",
            "P5: Intellectual Honesty\n",
            "Information must be represented accurately, even when adopting a specific persona.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quality Principles - The foundation of good writing\n",
        "QUALITY_PRINCIPLES = \"\"\"\n",
        "FUNDAMENTAL PRINCIPLES OF QUALITY WRITING:\n",
        "\n",
        "P1: Evidentiary Support\n",
        "All claims must be directly traceable to the provided research material.\n",
        "\n",
        "P2: Clarity and Conciseness\n",
        "Writing must be precise, unambiguous, and free of unnecessary jargon.\n",
        "\n",
        "P3: Engaging Narrative\n",
        "The post must feature a strong hook, a logical flow, and a memorable conclusion.\n",
        "\n",
        "P4: Structural Integrity\n",
        "The output must be well-organized with a clear title, introduction, body, and conclusion.\n",
        "\n",
        "P5: Intellectual Honesty\n",
        "Information must be represented accurately, even when adopting a specific persona.\n",
        "\"\"\"\n",
        "\n",
        "print(\"üéØ Quality principles established:\")\n",
        "print(QUALITY_PRINCIPLES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90KErXRgnmUi"
      },
      "source": [
        "## Utility Classes\n",
        "\n",
        "Let's set up some utility classes for web searching and file management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_MXabOZ_nmUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fc4710-99a9-4859-b673-04d7193dedad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Web searcher initialized (using free DuckDuckGo search)\n",
            "üìÇ File manager ready\n"
          ]
        }
      ],
      "source": [
        "class FreeWebSearcher:\n",
        "    \"\"\"Simple web search using DuckDuckGo (no API key required).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def search(self, query: str, num_results: int = 5) -> List[SearchResult]:\n",
        "        \"\"\"Perform a free web search using DuckDuckGo.\"\"\"\n",
        "        try:\n",
        "            search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
        "            response = self.session.get(search_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            results = []\n",
        "\n",
        "            for result_div in soup.find_all('div', class_='result')[:num_results]:\n",
        "                try:\n",
        "                    title_elem = result_div.find('a', class_='result__a')\n",
        "                    snippet_elem = result_div.find('a', class_='result__snippet')\n",
        "\n",
        "                    if title_elem and snippet_elem:\n",
        "                        title = title_elem.get_text(strip=True)\n",
        "                        url = title_elem.get('href', '')\n",
        "                        snippet = snippet_elem.get_text(strip=True)\n",
        "\n",
        "                        if title and url and snippet:\n",
        "                            results.append(SearchResult(title=title, url=url, snippet=snippet))\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {e}\")\n",
        "            return []\n",
        "\n",
        "class FileManager:\n",
        "    \"\"\"Manages file operations for the blog writing project.\"\"\"\n",
        "\n",
        "    def __init__(self, topic: str):\n",
        "        self.topic = topic\n",
        "        self.date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        # Create simple folder name\n",
        "        topic_abbrev = re.sub(r'[^a-zA-Z0-9]', '_', topic.lower())[:20]\n",
        "        self.folder_name = f\"{self.date_str}_{topic_abbrev}\"\n",
        "        self.output_dir = Path(self.folder_name)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        print(f\"üìÅ Created output directory: {self.output_dir}\")\n",
        "\n",
        "    def save_file(self, content: str, filename: str) -> Path:\n",
        "        filepath = self.output_dir / filename\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        print(f\"üíæ Saved: {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "# Initialize web searcher\n",
        "web_searcher = FreeWebSearcher()\n",
        "print(\"üîç Web searcher initialized (using free DuckDuckGo search)\")\n",
        "print(\"üìÇ File manager ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEx35vifnmUj"
      },
      "source": [
        "---\n",
        "\n",
        "# üéØ Exercise 1: Persona Architect Agent\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand how to create specialized agent personas\n",
        "- Learn to generate targeted search queries\n",
        "- Practice JSON output formatting for agent communication\n",
        "\n",
        "## Background\n",
        "The **Persona Architect** is your first agent. Its job is to:\n",
        "1. Analyze the topic and style requirements\n",
        "2. Create a detailed writer persona\n",
        "3. Generate search queries for research\n",
        "\n",
        "This follows the **ReAct** pattern:\n",
        "- **Thought**: \"I need to create a persona suitable for this topic\"\n",
        "- **Action**: Generate persona and queries using the LLM\n",
        "- **Observation**: Return structured results for the next agent\n",
        "\n",
        "## Your Task\n",
        "Complete the `PersonaArchitect` class by implementing the `generate_persona_and_queries` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9MeGtVpknmUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c1e5127d-b9a3-487a-851a-07f5b283f42a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé≠ PersonaArchitect class defined\n",
            "‚ö†Ô∏è  TODO: Implement the generate_persona_and_queries method\n",
            "‚úÖ Exercise 1 completed successfully!\n",
            "üìù Generated persona: You are a PhD candidate in Quantitative Finance and Econometrics, skilled at demystifying complex to...\n",
            "üîç Generated 5 search queries: ['using LLMs for sentiment analysis of financial and political news to predict market volatility', 'econometric models for incorporating unstructured news data into financial risk forecasting', 'quantitative analysis of geopolitical news impact on financial market risk metrics', 'natural language processing (NLP) applications in quantitative finance for risk management', 'case studies of LLM-driven analysis of economic news streams for alpha generation and risk estimation']\n"
          ]
        }
      ],
      "source": [
        "class PersonaArchitect:\n",
        "    \"\"\"Agent that creates writer personas and search strategies.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def generate_persona_and_queries(self, topic: str, style_and_background: str) -> PersonaResult:\n",
        "        \"\"\"Generate a writer persona and search queries for the given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: The blog post topic\n",
        "            style_and_background: Style guide and background requirements\n",
        "\n",
        "        Returns:\n",
        "            PersonaResult with persona_prompt and search_queries\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        As the Persona Architect, your task is to create a detailed writer persona and relevant search queries\n",
        "        for a blog post based on the given TOPIC and STYLE REQUIREMENTS.\n",
        "\n",
        "        The writer persona should be a brief description (1-3 sentences) that captures the\n",
        "        writing style, expertise, and perspective needed for this blog post.\n",
        "\n",
        "        You should also generate at least 3 diverse and targeted search queries that\n",
        "        a Research Analyst can use to find current and relevant information about the topic.\n",
        "\n",
        "        Provide the output in JSON format with the following structure:\n",
        "        {{\n",
        "          \"persona_prompt\": \"Detailed writer persona description\",\n",
        "          \"search_queries\": [\"query 1\", \"query 2\", \"query 3\", ...]\n",
        "        }}\n",
        "\n",
        "        Ensure the JSON is valid and can be directly parsed. Do not include any other text outside the JSON.\n",
        "\n",
        "        TOPIC: {topic}\n",
        "        STYLE REQUIREMENTS: {style_and_background}\n",
        "\n",
        "        # User's specific prompt instructions:\n",
        "        You are an AI assistant, work as a phd student writing paper in quantitative finance and econometrics\n",
        "        1. search elements in economics to estimate the risks in the quantitative finance market,\n",
        "        search use political news, financial news, economics news\n",
        "        use LLMs to analyze streams of political news, financial news, economics news to increase the accuracy in the risk estimation\n",
        "        2. use chart, graph, for the regression and analyze data and information in words, numbers, etc.\n",
        "        \"\"\"\n",
        "\n",
        "        response_text = \"\" # Initialize response_text before the try block\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            response_text = response.text.strip()\n",
        "\n",
        "            # Clean the response text to ensure it's valid JSON\n",
        "            # Remove potential markdown code block wrappers\n",
        "            if response_text.startswith(\"```json\"):\n",
        "                response_text = response_text[7:]\n",
        "            if response_text.endswith(\"```\"):\n",
        "                response_text = response_text[:-3]\n",
        "            response_text = response_text.strip()\n",
        "\n",
        "\n",
        "            data = json.loads(response_text)\n",
        "\n",
        "            return PersonaResult(\n",
        "                persona_prompt=data.get(\"persona_prompt\", \"\"),\n",
        "                search_queries=data.get(\"search_queries\", [])\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in PersonaArchitect: {e}\")\n",
        "            # Attempt to extract persona and queries even if JSON parsing fails partially\n",
        "            # This is a fallback for robustness, proper JSON is preferred\n",
        "            fallback_persona = \"Default writer persona.\"\n",
        "            fallback_queries = [f\"research for {topic}\"]\n",
        "            if \"persona_prompt\" in response_text:\n",
        "                 match = re.search(r'\"persona_prompt\"\\s*:\\s*\"(.*?)\"', response_text, re.DOTALL)\n",
        "                 if match:\n",
        "                     fallback_persona = match.group(1)\n",
        "            if \"search_queries\" in response_text:\n",
        "                 match = re.search(r'\"search_queries\"\\s*:\\s*\\[(.*?)\\]', response_text, re.DOTALL)\n",
        "                 if match:\n",
        "                     queries_str = match.group(1)\n",
        "                     fallback_queries = [q.strip().strip('\"') for q in queries_str.split(',') if q.strip()]\n",
        "\n",
        "\n",
        "            print(\"‚ö†Ô∏è  Attempting to use fallback parsing for PersonaArchitect response.\")\n",
        "            return PersonaResult(\n",
        "                persona_prompt=fallback_persona,\n",
        "                search_queries=fallback_queries\n",
        "            )\n",
        "\n",
        "\n",
        "# Test your implementation\n",
        "print(\"üé≠ PersonaArchitect class defined\")\n",
        "print(\"‚ö†Ô∏è  TODO: Implement the generate_persona_and_queries method\")\n",
        "\n",
        "# Assertion to check if method is implemented\n",
        "persona_architect = PersonaArchitect(model)\n",
        "\n",
        "# This will fail until you implement the method properly\n",
        "try:\n",
        "    test_result = persona_architect.generate_persona_and_queries(\n",
        "        \"Paper of PHD level in quantitative finance and econometrics to estimate Economic Risks in Quantitative Finance market\",\n",
        "        \"Professional, accessible tone for general audience\",\n",
        "\n",
        "    )\n",
        "    assert isinstance(test_result.persona_prompt, str) and len(test_result.persona_prompt) > 10\n",
        "    assert isinstance(test_result.search_queries, list) and len(test_result.search_queries) >= 2 # Changed from 1 to 2 to reflect user's prompt\n",
        "    print(\"‚úÖ Exercise 1 completed successfully!\")\n",
        "    print(f\"üìù Generated persona: {test_result.persona_prompt[:100]}...\")\n",
        "    print(f\"üîç Generated {len(test_result.search_queries)} search queries: {test_result.search_queries}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Exercise 1 incomplete: {e}\")\n",
        "    print(\"üí° Hint: Make sure to implement the method and return valid PersonaResult\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iivvqMqInmUk"
      },
      "source": [
        "---\n",
        "\n",
        "# üîç Exercise 2: Research Analyst Agent\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn to integrate external tools (web search) with LLMs\n",
        "- Practice data synthesis and consolidation\n",
        "- Understand how agents use tools to gather information\n",
        "\n",
        "## Background\n",
        "The **Research Analyst** uses the search queries from Exercise 1 to:\n",
        "1. Perform actual web searches\n",
        "2. Collect and analyze search results\n",
        "3. Synthesize findings into coherent research content\n",
        "\n",
        "This demonstrates how agents use **tools** (the search function) to interact with the world.\n",
        "\n",
        "## Your Task\n",
        "Complete the `ResearchAnalyst` class by implementing the research synthesis logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6PEBs1RtnmUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "205bd726-1e2a-4e5d-cc0f-1992602e9197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç ResearchAnalyst class defined\n",
            "‚ö†Ô∏è  TODO: Implement the conduct_research method\n",
            "üîç Starting web research...\n",
            "   Searching: 'AI healthcare applications 2024'\n",
            "   Searching: 'machine learning medical diagnosis'\n",
            "üìä Found 10 total search results\n",
            "‚úÖ Exercise 2 completed successfully!\n",
            "üìä Research content length: 2506 characters\n",
            "üîó Sources found: 10\n"
          ]
        }
      ],
      "source": [
        "class ResearchAnalyst:\n",
        "    \"\"\"Agent that conducts web research and synthesizes findings.\"\"\"\n",
        "\n",
        "    def __init__(self, model, web_searcher):\n",
        "        self.model = model\n",
        "        self.web_searcher = web_searcher\n",
        "\n",
        "    def conduct_research(self, search_queries: List[str]) -> ResearchResult:\n",
        "        \"\"\"Conduct web research and synthesize findings.\n",
        "\n",
        "        Args:\n",
        "            search_queries: List of search queries to execute\n",
        "\n",
        "        Returns:\n",
        "            ResearchResult with synthesized content and source count\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üîç Starting web research...\")\n",
        "        all_results = []\n",
        "\n",
        "        # Execute each search query and collect results\n",
        "        for i, query in enumerate(search_queries, 1):\n",
        "            print(f\"   Searching: '{query}'\")\n",
        "            results = self.web_searcher.search(query)\n",
        "            all_results.extend(results)\n",
        "\n",
        "            # Be respectful to the search service\n",
        "            if i < len(search_queries):\n",
        "                time.sleep(1)\n",
        "\n",
        "        print(f\"üìä Found {len(all_results)} total search results\")\n",
        "\n",
        "        if not all_results:\n",
        "            return ResearchResult(\n",
        "                content=\"--- RESEARCH START ---\\nNo search results found.\\n--- RESEARCH END ---\",\n",
        "                source_count=0\n",
        "            )\n",
        "\n",
        "        # Format search results for LLM analysis\n",
        "        search_content = \"\"\n",
        "        for i, result in enumerate(all_results, 1):\n",
        "            search_content += f\"--- SOURCE {i} ---\\n\"\n",
        "            search_content += f\"Title: {result.title}\\n\"\n",
        "            search_content += f\"URL: {result.url}\\n\"\n",
        "            search_content += f\"Snippet: {result.snippet}\\n\\n\"\n",
        "\n",
        "        # Create a prompt to analyze and synthesize the search results\n",
        "        analysis_prompt = f\"\"\"\n",
        "        As the Research Analyst, your task is to analyze the provided search results and synthesize them\n",
        "        into a comprehensive and coherent research summary for a blog post.\n",
        "\n",
        "        Focus on extracting key information, facts, and insights relevant to the blog post topic.\n",
        "        Do not just list the snippets; interpret and synthesize the information.\n",
        "\n",
        "        Ensure the synthesized content is accurate and directly supported by the search results.\n",
        "        Include citations or references to the source numbers (e.g., [Source 1]) where appropriate\n",
        "        to maintain evidentiary support.\n",
        "\n",
        "        Your output should be a detailed summary, wrapped in '--- RESEARCH START ---' and\n",
        "        '--- RESEARCH END ---' markers.\n",
        "\n",
        "        SEARCH RESULTS TO ANALYZE:\n",
        "        {search_content}\n",
        "\n",
        "        # Your synthesized research summary:\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(analysis_prompt)\n",
        "            content = response.text.strip()\n",
        "\n",
        "            return ResearchResult(\n",
        "                content=f\"--- RESEARCH START ---\\n{content}\\n--- RESEARCH END ---\",\n",
        "                source_count=len(all_results)\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error analyzing search results: {e}\")\n",
        "            # Return basic compilation as fallback\n",
        "            basic_content = f\"--- RESEARCH START ---\\nFound {len(all_results)} sources:\\n\"\n",
        "            for i, result in enumerate(all_results[:5]):  # Top 5\n",
        "                basic_content += f\"- [Source {i+1}] {result.title}: {result.snippet}\\n\"\n",
        "            basic_content += \"--- RESEARCH END ---\"\n",
        "\n",
        "            return ResearchResult(content=basic_content, source_count=len(all_results))\n",
        "\n",
        "# Test your implementation\n",
        "print(\"üîç ResearchAnalyst class defined\")\n",
        "print(\"‚ö†Ô∏è  TODO: Implement the conduct_research method\")\n",
        "\n",
        "# Test with the PersonaArchitect from Exercise 1\n",
        "research_analyst = ResearchAnalyst(model, web_searcher)\n",
        "\n",
        "# This will fail until both Exercise 1 and 2 are implemented\n",
        "try:\n",
        "    test_queries = [\"AI healthcare applications 2024\", \"machine learning medical diagnosis\"]\n",
        "    test_research = research_analyst.conduct_research(test_queries)\n",
        "\n",
        "    assert isinstance(test_research.content, str) and len(test_research.content) > 100\n",
        "    assert \"--- RESEARCH START ---\" in test_research.content\n",
        "    assert \"--- RESEARCH END ---\" in test_research.content\n",
        "    assert test_research.source_count >= 0\n",
        "\n",
        "    print(\"‚úÖ Exercise 2 completed successfully!\")\n",
        "    print(f\"üìä Research content length: {len(test_research.content)} characters\")\n",
        "    print(f\"üîó Sources found: {test_research.source_count}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Exercise 2 incomplete: {e}\")\n",
        "    print(\"üí° Hint: Make sure to implement web search execution and content synthesis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MjmorMvnmUl"
      },
      "source": [
        "---\n",
        "\n",
        "# ‚úçÔ∏è Exercise 3: Content Synthesizer Agent\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn to combine multiple inputs (persona + research) for content generation\n",
        "- Practice adherence to style guidelines and quality principles\n",
        "- Understand how to handle iterative improvement (revisions)\n",
        "\n",
        "## Background\n",
        "The **Content Synthesizer** is the creative heart of our system. It:\n",
        "1. Takes the persona and research from previous agents\n",
        "2. Writes blog posts that follow the quality principles\n",
        "3. Can revise content based on feedback (iterative improvement)\n",
        "\n",
        "This demonstrates how agents can work with complex, multi-part inputs.\n",
        "\n",
        "## Your Task\n",
        "Complete the `ContentSynthesizer` class by implementing the blog writing logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k0kOr2WHnmUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d1b98c8c-e85a-47f3-92e4-7c5d17634478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úçÔ∏è ContentSynthesizer class defined\n",
            "‚ö†Ô∏è  TODO: Implement the write_blog_post method\n",
            "‚úÖ Exercise 3 completed successfully!\n",
            "üìù Blog post length: 2722 characters\n",
            "üìÑ Version: 1\n"
          ]
        }
      ],
      "source": [
        "class ContentSynthesizer:\n",
        "    \"\"\"Agent that writes blog posts based on persona and research.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def write_blog_post(self, persona_prompt: str, research_content: str,\n",
        "                       topic: str, style_requirements: str,\n",
        "                       editorial_feedback: Optional[str] = None) -> BlogDraft:\n",
        "        \"\"\"Write a blog post using persona and research.\n",
        "\n",
        "        Args:\n",
        "            persona_prompt: The writer persona to adopt\n",
        "            research_content: Synthesized research content\n",
        "            topic: The blog post topic\n",
        "            style_requirements: Style and background requirements\n",
        "            editorial_feedback: Optional feedback for revision\n",
        "\n",
        "        Returns:\n",
        "            BlogDraft with content and version number\n",
        "        \"\"\"\n",
        "\n",
        "        base_prompt = f\"\"\"\n",
        "        As the Content Synthesizer, your task is to write a blog post based on the provided\n",
        "        WRITER PERSONA, RESEARCH CONTENT, TOPIC, and STYLE REQUIREMENTS.\n",
        "\n",
        "        Adhere strictly to the following QUALITY PRINCIPLES. Ensure all claims are supported by the research.\n",
        "        Structure the post with a clear title, introduction, body, and conclusion, using Markdown format.\n",
        "        The post should be approximately 500 words.\n",
        "\n",
        "        WRITER PERSONA:\n",
        "        {persona_prompt}\n",
        "\n",
        "        RESEARCH CONTENT:\n",
        "        {research_content}\n",
        "\n",
        "        TOPIC: {topic}\n",
        "\n",
        "        STYLE REQUIREMENTS:\n",
        "        {style_requirements}\n",
        "\n",
        "        QUALITY PRINCIPLES TO FOLLOW:\n",
        "        {QUALITY_PRINCIPLES}\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if editorial_feedback:\n",
        "            base_prompt += f\"\"\"\n",
        "\n",
        "            EDITORIAL FEEDBACK TO ADDRESS:\n",
        "            {editorial_feedback}\n",
        "\n",
        "            Carefully review the feedback and revise the previous draft to improve its quality\n",
        "            and address all the points raised. Provide the revised draft.\n",
        "            \"\"\"\n",
        "            version = 2 # Assume revision if feedback is provided\n",
        "        else:\n",
        "             version = 1 # Initial draft\n",
        "\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(base_prompt)\n",
        "            content = response.text.strip()\n",
        "\n",
        "            return BlogDraft(content=content, version=version)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in ContentSynthesizer: {e}\")\n",
        "            raise\n",
        "\n",
        "# Test your implementation\n",
        "print(\"‚úçÔ∏è ContentSynthesizer class defined\")\n",
        "print(\"‚ö†Ô∏è  TODO: Implement the write_blog_post method\")\n",
        "\n",
        "# Test with sample data\n",
        "content_synthesizer = ContentSynthesizer(model)\n",
        "\n",
        "# This will fail until you implement the method\n",
        "try:\n",
        "    sample_persona = \"Expert technology writer with 10 years of experience in AI and healthcare.\"\n",
        "    sample_research = \"--- RESEARCH START ---\\nAI is transforming healthcare through improved diagnostics and personalized treatment.\\n--- RESEARCH END ---\"\n",
        "\n",
        "    test_draft = content_synthesizer.write_blog_post(\n",
        "        persona_prompt=sample_persona,\n",
        "        research_content=sample_research,\n",
        "        topic=\"AI in Healthcare\",\n",
        "        style_requirements=\"Professional, accessible tone\"\n",
        "    )\n",
        "\n",
        "    assert isinstance(test_draft.content, str) and len(test_draft.content) > 200\n",
        "    assert test_draft.version == 1\n",
        "    assert \"#\" in test_draft.content  # Should have Markdown title\n",
        "\n",
        "    print(\"‚úÖ Exercise 3 completed successfully!\")\n",
        "    print(f\"üìù Blog post length: {len(test_draft.content)} characters\")\n",
        "    print(f\"üìÑ Version: {test_draft.version}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Exercise 3 incomplete: {e}\")\n",
        "    print(\"üí° Hint: Make sure to implement blog writing logic with all required inputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK6EuaRqnmUm"
      },
      "source": [
        "---\n",
        "\n",
        "# üîç Exercise 4: Critic Agent\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn to evaluate content against specific criteria\n",
        "- Practice providing structured feedback\n",
        "- Understand quality assessment in AI systems\n",
        "\n",
        "## Background\n",
        "The **Critic Agent** evaluates blog drafts against our quality principles. It:\n",
        "1. Analyzes the draft for adherence to quality principles (P1-P5)\n",
        "2. Checks factual consistency with research\n",
        "3. Provides structured feedback with specific issues\n",
        "\n",
        "This is the \"evaluator\" part of the **Evaluator-Optimizer** pattern you learned about.\n",
        "\n",
        "## Your Task\n",
        "Complete the `CriticAgent` class by implementing the quality evaluation logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "r0M60ElFnmUm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "dd7801d6-bc22-48f3-a8dd-bf24f30e6808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç CriticAgent class defined\n",
            "‚ö†Ô∏è  TODO: Implement the evaluate_draft method\n",
            "‚úÖ Exercise 4 completed successfully!\n",
            "‚úÖ Approved: False\n",
            "üìù Feedback length: 1146 characters\n",
            "‚ö†Ô∏è  Issues found: 3\n"
          ]
        }
      ],
      "source": [
        "class CriticAgent:\n",
        "    \"\"\"Agent that evaluates blog drafts against quality principles.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def evaluate_draft(self, draft_content: str, research_content: str,\n",
        "                      topic: str) -> QualityReview:\n",
        "        \"\"\"Evaluate a blog draft against quality principles.\n",
        "\n",
        "        Args:\n",
        "            draft_content: The blog draft to evaluate\n",
        "            research_content: Original research content for fact-checking\n",
        "            topic: The blog post topic\n",
        "\n",
        "        Returns:\n",
        "            QualityReview with approval status, feedback, and issues\n",
        "        \"\"\"\n",
        "\n",
        "        evaluation_prompt = f\"\"\"\n",
        "        As the Critic Agent, your task is to rigorously evaluate the provided BLOG DRAFT against the\n",
        "        RESEARCH CONTENT and the defined QUALITY PRINCIPLES.\n",
        "\n",
        "        Analyze the draft for adherence to each of the following principles:\n",
        "        {QUALITY_PRINCIPLES}\n",
        "\n",
        "        Specifically, check for:\n",
        "        - **P1: Evidentiary Support:** Are all claims in the draft directly supported by the research content? Point out any claims lacking support.\n",
        "        - **P2: Clarity and Conciseness:** Is the writing clear, unambiguous, and free of jargon? Suggest areas for improvement.\n",
        "        - **P3: Engaging Narrative:** Does it have a strong hook, logical flow, and conclusion? Provide specific feedback on engagement.\n",
        "        - **P4: Structural Integrity:** Is it well-organized with a clear title, intro, body, and conclusion using Markdown? Note any structural issues.\n",
        "        - **P5: Intellectual Honesty:** Is the information represented accurately based on the research, even if adopting a persona? Identify any misrepresentations.\n",
        "\n",
        "        Based on your evaluation, determine if the draft is APPROVED. It should only be approved if it meets all fundamental quality principles with no significant issues.\n",
        "\n",
        "        Provide your evaluation and decision in JSON format with the following structure:\n",
        "        {{\n",
        "          \"is_approved\": true/false,\n",
        "          \"feedback\": \"Detailed feedback on strengths and weaknesses, referencing specific principles (e.g., 'P1 Issue: Claim X is not supported by Source Y'). Provide actionable suggestions for improvement.\",\n",
        "          \"issues_found\": [\"Brief summary of issue 1 (e.g., P1: Lack of evidence)\", \"Brief summary of issue 2\", ...]\n",
        "        }}\n",
        "\n",
        "        Ensure the JSON is valid and can be directly parsed. Do not include any other text outside the JSON.\n",
        "\n",
        "        BLOG DRAFT TO EVALUATE:\n",
        "        {draft_content}\n",
        "\n",
        "        RESEARCH CONTENT FOR FACT-CHECKING:\n",
        "        {research_content}\n",
        "\n",
        "        TOPIC: {topic}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(evaluation_prompt)\n",
        "            response_text = response.text.strip()\n",
        "\n",
        "            # Clean and parse JSON response\n",
        "            if response_text.startswith(\"```json\"):\n",
        "                response_text = response_text[7:]\n",
        "            if response_text.endswith(\"```\"):\n",
        "                response_text = response_text[:-3]\n",
        "            response_text = response_text.strip()\n",
        "\n",
        "            data = json.loads(response_text)\n",
        "\n",
        "            return QualityReview(\n",
        "                is_approved=data.get(\"is_approved\", False),\n",
        "                feedback=data.get(\"feedback\", \"No feedback provided.\"),\n",
        "                issues_found=data.get(\"issues_found\", [])\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in CriticAgent: {e}\")\n",
        "            # Return safe fallback\n",
        "            return QualityReview(\n",
        "                is_approved=False,\n",
        "                feedback=f\"Error during evaluation: {e}\",\n",
        "                issues_found=[\"Evaluation error occurred\"]\n",
        "            )\n",
        "\n",
        "# Test your implementation\n",
        "print(\"üîç CriticAgent class defined\")\n",
        "print(\"‚ö†Ô∏è  TODO: Implement the evaluate_draft method\")\n",
        "\n",
        "# Test with sample data\n",
        "critic_agent = CriticAgent(model)\n",
        "\n",
        "# This will fail until you implement the method\n",
        "try:\n",
        "    sample_draft = \"# AI in Healthcare\\n\\nArtificial Intelligence is transforming healthcare in many ways...\"\n",
        "    sample_research = \"--- RESEARCH START ---\\nAI applications include medical imaging and drug discovery.\\n--- RESEARCH END ---\"\n",
        "\n",
        "    test_review = critic_agent.evaluate_draft(\n",
        "        draft_content=sample_draft,\n",
        "        research_content=sample_research,\n",
        "        topic=\"AI in Healthcare\"\n",
        "    )\n",
        "\n",
        "    assert isinstance(test_review.is_approved, bool)\n",
        "    assert isinstance(test_review.feedback, str) and len(test_review.feedback) > 10\n",
        "    assert isinstance(test_review.issues_found, list)\n",
        "\n",
        "    print(\"‚úÖ Exercise 4 completed successfully!\")\n",
        "    print(f\"‚úÖ Approved: {test_review.is_approved}\")\n",
        "    print(f\"üìù Feedback length: {len(test_review.feedback)} characters\")\n",
        "    print(f\"‚ö†Ô∏è  Issues found: {len(test_review.issues_found)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Exercise 4 incomplete: {e}\")\n",
        "    print(\"üí° Hint: Make sure to implement evaluation logic with JSON output parsing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWperbUHnmUn"
      },
      "source": [
        "---\n",
        "\n",
        "# ‚úèÔ∏è Exercise 5: Editor Agent & Workflow Orchestration\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn to orchestrate multiple agents in a workflow\n",
        "- Implement iterative improvement loops\n",
        "- Understand the complete **Evaluator-Optimizer** pattern\n",
        "\n",
        "## Background\n",
        "The **Editor Agent** manages the iterative improvement process. It:\n",
        "1. Coordinates between the Content Synthesizer and Critic\n",
        "2. Manages the revision loop (up to 3 cycles)\n",
        "3. Makes final approval decisions\n",
        "\n",
        "This demonstrates the complete **Evaluator-Optimizer** workflow from your course!\n",
        "\n",
        "## Your Task\n",
        "Complete the `EditorAgent` class and implement the main workflow orchestration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Am256lXXnmUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e3509e-29d5-40e4-bf12-24b1b68dfe73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úèÔ∏è EditorAgent and BlogWorkflowOrchestrator classes defined\n",
            "‚ö†Ô∏è  TODO: Complete all agent implementations and orchestration logic\n",
            "‚úÖ Exercise 5 setup completed successfully!\n",
            "üéâ All agents initialized and ready for workflow execution\n"
          ]
        }
      ],
      "source": [
        "class EditorAgent:\n",
        "    \"\"\"Agent that manages the iterative improvement workflow.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def make_editorial_decision(self, quality_review: QualityReview,\n",
        "                               current_cycle: int, max_cycles: int) -> EditorReview:\n",
        "        \"\"\"Make an editorial decision based on critic feedback.\n",
        "\n",
        "        Args:\n",
        "            quality_review: Review from the Critic Agent\n",
        "            current_cycle: Current revision cycle number\n",
        "            max_cycles: Maximum allowed cycles\n",
        "\n",
        "        Returns:\n",
        "            EditorReview with final decision and comments\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement editorial decision logic\n",
        "        if quality_review.is_approved:\n",
        "            # Handle approval case\n",
        "            return EditorReview(\n",
        "                is_approved=True,\n",
        "                comments=\"üéâ The draft has been approved by the Critic Agent. No further revisions needed.\"\n",
        "            )\n",
        "        elif current_cycle >= max_cycles:\n",
        "            # Handle max cycles reached\n",
        "            return EditorReview(\n",
        "                is_approved=True,  # Approve despite issues\n",
        "                comments=f\"‚ö†Ô∏è Maximum revision cycles ({max_cycles}) reached. Approving the current draft despite remaining issues:\\n{quality_review.feedback}\"\n",
        "            )\n",
        "        else:\n",
        "            # Handle revision needed case\n",
        "            return EditorReview(\n",
        "                is_approved=False,\n",
        "                comments=f\"üîÑ Revision needed. Please address the following feedback from the Critic Agent:\\n{quality_review.feedback}\"\n",
        "            )\n",
        "\n",
        "class BlogWorkflowOrchestrator:\n",
        "    \"\"\"Main orchestrator that coordinates all agents in the workflow.\"\"\"\n",
        "\n",
        "    def __init__(self, model, web_searcher):\n",
        "        # Initialize all agents\n",
        "        self.persona_architect = PersonaArchitect(model)\n",
        "        self.research_analyst = ResearchAnalyst(model, web_searcher)\n",
        "        self.content_synthesizer = ContentSynthesizer(model)\n",
        "        self.critic_agent = CriticAgent(model)\n",
        "        self.editor_agent = EditorAgent(model)\n",
        "\n",
        "    def generate_blog_post(self, topic: str, style_and_background: str) -> Tuple[str, Path]:\n",
        "        \"\"\"Execute the complete blog generation workflow.\n",
        "\n",
        "        Args:\n",
        "            topic: Blog post topic\n",
        "            style_and_background: Style requirements\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (final_content, final_file_path)\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"üöÄ Starting blog generation for: '{topic}'\")\n",
        "\n",
        "        # Initialize file manager\n",
        "        file_manager = FileManager(topic)\n",
        "\n",
        "        try:\n",
        "            # Phase 1: Generate Persona and Queries\n",
        "            print(\"\\nüé≠ Phase 1: Generating writer persona and search queries...\")\n",
        "            persona_result = self.persona_architect.generate_persona_and_queries(topic, style_and_background)\n",
        "\n",
        "            print(f\"   Generated persona with {len(persona_result.search_queries)} search queries\")\n",
        "\n",
        "            # Phase 2: Conduct Research\n",
        "            print(\"\\nüîç Phase 2: Conducting web research...\")\n",
        "            research_result = self.research_analyst.conduct_research(persona_result.search_queries)\n",
        "\n",
        "            print(f\"   Research completed with {research_result.source_count} sources\")\n",
        "\n",
        "            # Phase 3: Iterative Content Generation and Review\n",
        "            print(\"\\n‚úçÔ∏è Phase 3: Starting iterative content creation...\")\n",
        "\n",
        "            current_draft = None\n",
        "            max_cycles = 3\n",
        "            last_feedback = None\n",
        "\n",
        "            for cycle in range(1, max_cycles + 1):\n",
        "                print(f\"\\n   üìù Cycle {cycle}/{max_cycles}\")\n",
        "\n",
        "                # Generate or revise draft\n",
        "                if current_draft is None:\n",
        "                    print(\"      Writing initial draft...\")\n",
        "                    draft = self.content_synthesizer.write_blog_post(\n",
        "                        persona_prompt=persona_result.persona_prompt,\n",
        "                        research_content=research_result.content,\n",
        "                        topic=topic,\n",
        "                        style_requirements=style_and_background\n",
        "                    )\n",
        "                else:\n",
        "                    print(\"      Revising draft based on feedback...\")\n",
        "                    draft = self.content_synthesizer.write_blog_post(\n",
        "                        persona_prompt=persona_result.persona_prompt,\n",
        "                        research_content=research_result.content,\n",
        "                        topic=topic,\n",
        "                        style_requirements=style_and_background,\n",
        "                        editorial_feedback=last_feedback\n",
        "                    )\n",
        "\n",
        "                current_draft = draft\n",
        "\n",
        "                # Save draft\n",
        "                file_manager.save_file(draft.content, f\"draft_{cycle}.md\")\n",
        "\n",
        "                # Evaluate with Critic\n",
        "                print(\"      Evaluating with Critic Agent...\")\n",
        "                quality_review = self.critic_agent.evaluate_draft(\n",
        "                    draft_content=current_draft.content,\n",
        "                    research_content=research_result.content,\n",
        "                    topic=topic\n",
        "                )\n",
        "\n",
        "                # Make editorial decision\n",
        "                print(\"      Making editorial decision...\")\n",
        "                editorial_review = self.editor_agent.make_editorial_decision(\n",
        "                    quality_review=quality_review,\n",
        "                    current_cycle=cycle,\n",
        "                    max_cycles=max_cycles\n",
        "                )\n",
        "\n",
        "                # Save review\n",
        "                file_manager.save_file(\n",
        "                    f\"Approved: {editorial_review.is_approved}\\n\\n{editorial_review.comments}\",\n",
        "                    f\"review_{cycle}.md\"\n",
        "                )\n",
        "\n",
        "                print(f\"      Result: {'‚úÖ Approved' if editorial_review.is_approved else 'üîÑ Needs revision'}\")\n",
        "\n",
        "                if editorial_review.is_approved:\n",
        "                    print(f\"   üéâ Blog post approved after {cycle} cycle(s)!\")\n",
        "                    break\n",
        "\n",
        "                # Store feedback for next revision\n",
        "                last_feedback = editorial_review.comments\n",
        "\n",
        "            # Phase 4: Finalization\n",
        "            print(\"\\nüìÑ Phase 4: Finalizing blog post...\")\n",
        "            final_path = file_manager.save_file(current_draft.content, \"final_blog.md\")\n",
        "\n",
        "            # Display summary\n",
        "            word_count = len(current_draft.content.split())\n",
        "            print(f\"\\nüìä Generation Summary:\")\n",
        "            print(f\"   üìÇ Output folder: {file_manager.folder_name}\")\n",
        "            print(f\"   üìù Final word count: {word_count} words\")\n",
        "            print(f\"   üîç Sources used: {research_result.source_count}\")\n",
        "            print(f\"   üîÑ Revision cycles: {cycle}\")\n",
        "\n",
        "            return current_draft.content, final_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during blog generation: {e}\")\n",
        "            raise\n",
        "\n",
        "# Test your implementation\n",
        "print(\"‚úèÔ∏è EditorAgent and BlogWorkflowOrchestrator classes defined\")\n",
        "print(\"‚ö†Ô∏è  TODO: Complete all agent implementations and orchestration logic\")\n",
        "\n",
        "# This will fail until all exercises are completed\n",
        "try:\n",
        "    orchestrator = BlogWorkflowOrchestrator(model, web_searcher)\n",
        "\n",
        "    # Verify all agents are initialized\n",
        "    assert orchestrator.persona_architect is not None\n",
        "    assert orchestrator.research_analyst is not None\n",
        "    assert orchestrator.content_synthesizer is not None\n",
        "    assert orchestrator.critic_agent is not None\n",
        "    assert orchestrator.editor_agent is not None\n",
        "\n",
        "    print(\"‚úÖ Exercise 5 setup completed successfully!\")\n",
        "    print(\"üéâ All agents initialized and ready for workflow execution\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Exercise 5 incomplete: {e}\")\n",
        "    print(\"üí° Hint: Make sure all previous exercises are completed and all agents are initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUgHPFFonmUn"
      },
      "source": [
        "---\n",
        "\n",
        "# üéØ Final Integration Test\n",
        "\n",
        "Once you've completed all 5 exercises, run this cell to test the complete workflow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "W4elYqaCnmUn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "809e10a6-5959-417c-ac7d-a8a3a1d2d5ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Ready to run integration test...\n",
            "‚ö†Ô∏è  Make sure all exercises above are completed first!\n",
            "üöÄ Starting blog generation for: 'Risks in Finance and Economics and How Risks Inference Quantitative Finance Market'\n",
            "üìÅ Created output directory: 20250821_risks_in_finance_and\n",
            "\n",
            "üé≠ Phase 1: Generating writer persona and search queries...\n",
            "   Generated persona with 5 search queries\n",
            "\n",
            "üîç Phase 2: Conducting web research...\n",
            "üîç Starting web research...\n",
            "   Searching: 'Econometric models for market risk prediction using LLM-based news sentiment analysis'\n",
            "   Searching: 'Case studies on using NLP and LLMs to quantify the impact of political news on quantitative finance risk metrics (VaR, ES)'\n",
            "   Searching: 'Challenges and opportunities of integrating real-time textual data from financial and economic news into quantitative risk management frameworks'\n",
            "   Searching: 'Time-series analysis of market volatility using textual data from financial news feeds'\n",
            "   Searching: 'Using transformer models to forecast financial crises from streams of economic and political news reports'\n",
            "üìä Found 25 total search results\n",
            "   Research completed with 25 sources\n",
            "\n",
            "‚úçÔ∏è Phase 3: Starting iterative content creation...\n",
            "\n",
            "   üìù Cycle 1/3\n",
            "      Writing initial draft...\n",
            "‚ùå Error in ContentSynthesizer: 500 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
            "‚ùå Error during blog generation: 500 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
            "‚ùå Integration test failed: 500 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
            "\n",
            "üîß Please review and complete all exercises above.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:500 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1315.95ms\n"
          ]
        }
      ],
      "source": [
        "# Final integration test - run the complete workflow\n",
        "def run_complete_workflow():\n",
        "    \"\"\"Run the complete blog generation workflow.\"\"\"\n",
        "\n",
        "    # Sample inputs\n",
        "    topic = \"Risks in Finance and Economics and How Risks Inference Quantitative Finance Market\"\n",
        "    style_and_background = \"\"\"\n",
        "    Target audience: College professors who reads the finance and econometrics paper\n",
        "    Tone: Professional but accessible, optimistic yet balanced\n",
        "    Style: Informative with real-world examples, approximately 500 words\n",
        "    Background: Write from the perspective of someone knowledgeable about both quantitative finance and econometrics\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Initialize orchestrator\n",
        "        orchestrator = BlogWorkflowOrchestrator(model, web_searcher)\n",
        "\n",
        "        # Run complete workflow\n",
        "        final_content, final_path = orchestrator.generate_blog_post(topic, style_and_background)\n",
        "\n",
        "        # Display final result\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üéâ CAPSTONE PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Show preview of final blog\n",
        "        preview = final_content[:500] + \"...\" if len(final_content) > 500 else final_content\n",
        "        display(Markdown(f\"### Final Blog Post Preview\\n\\n{preview}\"))\n",
        "\n",
        "        print(f\"\\nüìÑ Full blog saved to: {final_path}\")\n",
        "        print(\"\\nüèÜ Congratulations! You've successfully built a multi-agent system!\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Integration test failed: {e}\")\n",
        "        print(\"\\nüîß Please review and complete all exercises above.\")\n",
        "        return False\n",
        "\n",
        "# Run the test when all exercises are complete\n",
        "print(\"üß™ Ready to run integration test...\")\n",
        "print(\"‚ö†Ô∏è  Make sure all exercises above are completed first!\")\n",
        "\n",
        "# Uncomment the next line when you're ready to test:\n",
        "success = run_complete_workflow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c4fcfc4"
      },
      "source": [
        "!git config --global user.email \"isabelqingou@gmail.com\"\n",
        "!git config --global user.name \"QO2021\""
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04a81df4",
        "outputId": "cb4475d1-eb50-4db8-9868-b9e5738ba127"
      },
      "source": [
        "# Push the changes to the remote repository\n",
        "!git push -u origin master"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80fbca61",
        "outputId": "d685f6a9-7622-4c83-95ef-3d663eb2cf34"
      },
      "source": [
        "# Add the remote origin (replace with your repository URL if different)\n",
        "!git remote add origin https://github.com/QO2021/Blog.git"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: remote origin already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39627a41",
        "outputId": "a3379c30-6ca1-437d-ae03-c11adef81f74"
      },
      "source": [
        "# Commit the changes\n",
        "!git commit -m \"Add generated blog post and related files\""
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master (root-commit) 788cba4] Add generated blog post and related files\n",
            " 2 files changed, 27 insertions(+)\n",
            " create mode 100644 draft_1.md\n",
            " create mode 100644 review_1.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc47daeb"
      },
      "source": [
        "# Add all files in the directory to the staging area\n",
        "!git add ."
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e68f09f",
        "outputId": "3775c2d0-0224-4d05-dce1-eb8cf22ec813"
      },
      "source": [
        "# Initialize a Git repository (if not already initialized)\n",
        "!git init"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/20250821_risks_in_finance_and/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eb58ed8",
        "outputId": "a494d9f1-a27c-42d8-eeb4-72a6b982a484"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Get the most recent output directory name\n",
        "output_dirs = sorted(glob.glob('20250821_risks_in_finance_and'), key=os.path.getmtime, reverse=True)\n",
        "if output_dirs:\n",
        "    output_dir = output_dirs[0]\n",
        "    print(f\"Navigating to output directory: {output_dir}\")\n",
        "    %cd {output_dir}\n",
        "else:\n",
        "    print(\"Could not find an output directory starting with '20*_risks_in_finance_and'\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find an output directory starting with '20*_risks_in_finance_and'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfmlAw4TnmUo"
      },
      "source": [
        "---\n",
        "\n",
        "# üéì Capstone Project Summary\n",
        "\n",
        "## What You've Built\n",
        "\n",
        "Congratulations! You've just built a complete **multi-agent system** that demonstrates all the key concepts from your AI Agents course:\n",
        "\n",
        "### üß† Agent Fundamentals Applied\n",
        "- **Brain (LLM)**: Each agent uses Gemini Pro 2.5 for reasoning\n",
        "- **Tools**: Web search, file management, content generation\n",
        "- **Result**: 5 specialized agents working together\n",
        "\n",
        "### üîÑ ReAct Framework in Action\n",
        "- **Thought**: Each agent reasons about its specific task\n",
        "- **Action**: Agents use tools (search, generate, evaluate)\n",
        "- **Observation**: Results feed into the next agent or iteration\n",
        "\n",
        "### üè≠ Evaluator-Optimizer Workflow\n",
        "- **Creator**: Content Synthesizer writes drafts\n",
        "- **Evaluator**: Critic Agent reviews against quality principles\n",
        "- **Optimizer**: Editor Agent manages iterative improvement\n",
        "- **Loop**: Up to 3 cycles of continuous improvement\n",
        "\n",
        "## Technical Achievements\n",
        "\n",
        "‚úÖ **Exercise 1**: Persona generation with targeted search queries  \n",
        "‚úÖ **Exercise 2**: Real web search integration and research synthesis  \n",
        "‚úÖ **Exercise 3**: Content generation with persona adherence  \n",
        "‚úÖ **Exercise 4**: Quality evaluation against structured principles  \n",
        "‚úÖ **Exercise 5**: Complete workflow orchestration with iterative improvement\n",
        "\n",
        "## Real-World Applications\n",
        "\n",
        "The patterns you've learned can be applied to:\n",
        "- **Content Creation**: Automated writing with quality control\n",
        "- **Research & Analysis**: Multi-step investigation workflows\n",
        "- **Code Review**: Automated code evaluation and improvement\n",
        "- **Decision Support**: Multi-agent consultation systems\n",
        "- **Creative Projects**: AI collaboration in design and writing\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now that you understand multi-agent systems, consider exploring:\n",
        "- **Advanced Workflows**: Try parallel agent execution\n",
        "- **Specialized Tools**: Add more sophisticated tools to your agents\n",
        "- **Domain Expertise**: Create agents for specific industries\n",
        "- **Interactive Systems**: Build agents that can collaborate with humans\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Course Connection\n",
        "\n",
        "This capstone perfectly demonstrates the progression from your course:\n",
        "\n",
        "1. **Session 1**: Prompt Engineering ‚Üí Used in every agent interaction\n",
        "2. **Session 2**: RAG ‚Üí Research integration and knowledge synthesis  \n",
        "3. **Session 3**: AI Agents ‚Üí Complete multi-agent system with tools\n",
        "\n",
        "You've now experienced the full power of giving AI not just knowledge, but the ability to **act**!\n",
        "\n",
        "---\n",
        "\n",
        "üéâ **Congratulations on completing your AI Agents Capstone Project!** üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}